{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install speechbrain datasets==2.17.0","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset, Audio\nfrom transformers import (AutoProcessor\n                          , SpeechT5ForTextToSpeech\n                          , Seq2SeqTrainingArguments\n                          , Seq2SeqTrainer\n                          , SpeechT5HifiGan)\nfrom collections import defaultdict\nfrom speechbrain.inference.speaker import EncoderClassifier\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nfrom functools import partial\n\nfrom huggingface_hub import notebook_login\nfrom IPython.display import Audio as play_audio","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"facebook/voxpopuli\"\n                       , \"lt\"\n                       , split=\"train\"\n                       , trust_remote_code=True\n                       , streaming=True)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = \"microsoft/speecht5_tts\"\nprocessor = AutoProcessor.from_pretrained(checkpoint)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampling_rate = processor.feature_extractor.sampling_rate\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = processor.tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_all_chars(batch):\n    all_text = \" \".join(batch[\"normalized_text\"])\n    vocab = list(set(all_text))\n    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n\n\nvocabs = dataset.map(\n    extract_all_chars,\n    batched=True,\n    batch_size=-1,\n    #keep_in_memory=True,\n    remove_columns=dataset.column_names,\n)\n\ndataset_vocab = set(j for i in vocabs for j in i[\"vocab\"])\ntokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_vocab - tokenizer_vocab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replacements = [\n    (\"ą\",\"a\")\n    , (\"č\", \"c\")\n    , (\"ė\", \"e\")\n    , (\"ę\", \"e\")\n    , (\"į\", \"j\")\n    , (\"š\", \"s\")\n    , (\"ū\", \"u\")\n    , (\"ų\", \"u\")\n    , (\"ž\", \"z\")\n]\ndef cleanup_text(inputs):\n    for src, dst in replacements:\n        inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n    return inputs\n\ndataset = dataset.map(cleanup_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"speaker_counts = defaultdict(int)\nfor x in dataset:\n    speaker_counts[x[\"speaker_id\"]] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.hist(speaker_counts.values(), bins=20)\nplt.ylabel(\"Speakers\")\nplt.xlabel(\"Examples\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nspeaker_model = EncoderClassifier.from_hparams(\n    source=spk_model_name,\n    run_opts={\"device\": device},\n    savedir=os.path.join(\"/tmp\", spk_model_name),\n)\n\n\ndef create_speaker_embedding(waveform):\n    with torch.no_grad():\n        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n    return speaker_embeddings","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(example):\n    audio = example[\"audio\"]\n\n    example = processor(\n        text=example[\"normalized_text\"],\n        audio_target=audio[\"array\"],\n        sampling_rate=audio[\"sampling_rate\"],\n        return_attention_mask=False,\n    )\n\n    # strip off the batch dimension\n    example[\"labels\"] = example[\"labels\"][0]\n\n    # use SpeechBrain to obtain x-vector\n    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n\n    return example","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_example = prepare_dataset(next(iter(dataset)))\nlist(processed_example.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_example[\"speaker_embeddings\"].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (16, 9))\nplt.imshow(processed_example[\"labels\"].T)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(prepare_dataset\n#                    , batched=True\n#                    , batch_size=-1\n                   , remove_columns=dataset.column_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.shuffle(seed=0)\nlength = sum(1 for _ in dataset)\ntrain_ratio = 0.9\ntrain_size = round(train_ratio * length)\ntrain_dataset = dataset.take(train_size)\ntest_dataset = dataset.skip(train_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass TTSDataCollatorWithPadding:\n    processor: Any\n\n    def __call__(\n        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n    ) -> Dict[str, torch.Tensor]:\n        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n\n        # collate the inputs and targets into a batch\n        batch = processor.pad(\n            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n        )\n\n        # replace padding with -100 to ignore loss correctly\n        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n        )\n\n        # not used during fine-tuning\n        del batch[\"decoder_attention_mask\"]\n\n        # round down target lengths to multiple of reduction factor\n        if model.config.reduction_factor > 1:\n            target_lengths = torch.tensor(\n                [len(feature[\"input_values\"]) for feature in label_features]\n            )\n            target_lengths = target_lengths.new(\n                [\n                    length - length % model.config.reduction_factor\n                    for length in target_lengths\n                ]\n            )\n            max_length = max(target_lengths)\n            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n\n        # also add in the speaker embeddings\n        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n\n        return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = TTSDataCollatorWithPadding(processor=processor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# disable cache during training since it's incompatible with gradient checkpointing\nmodel.config.use_cache = False\n\n# set language and task for generation and re-enable cache\nmodel.generate = partial(model.generate, use_cache=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = checkpoint.split(\"/\")[-1]\nhf_dir = f\"{model_name}_finetuned_voxpopuli_lt\"\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=hf_dir,\n    auto_find_batch_size=True,\n    gradient_accumulation_steps=2,\n    learning_rate=5e-5,\n    warmup_steps=50,\n    max_steps=400,\n    gradient_checkpointing=True,\n    fp16=True,\n    evaluation_strategy=\"steps\",\n    save_steps=100,\n    eval_steps=100,\n    logging_steps=100,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    greater_is_better=False,\n    label_names=[\"labels\"],\n    push_to_hub=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=data_collator,\n    tokenizer=processor,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SpeechT5ForTextToSpeech.from_pretrained(\n    \"jaymanvirk/speecht5_tts_finetuned_voxpopuli_lt\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example = next(iter(test_dataset))\nspeaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"Šis yra bandymo pranešimas. Tikrinama 400 žingsnių smulkiu nustatymu modelio kokybė.\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = processor(text=text, return_tensors=\"pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\nspeech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"play_audio(speech.numpy(), rate=16000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}